import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.vectorstores import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from datetime import datetime
from langchain.schema import Document


# === 1. Charger les variables d‚Äôenvironnement ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")
PINECONE_INDEX = os.getenv("PINECONE_INDEX")

# === 2. Initialiser le mod√®le ===
llm = ChatOpenAI(temperature=0.5, model="gpt-4o")

# === 3. Initialiser Pinecone retriever ===
retriever = Pinecone.from_existing_index(
    index_name=PINECONE_INDEX,
    embedding=OpenAIEmbeddings()
).as_retriever()


long_term_store = Pinecone.from_existing_index(
    index_name=PINECONE_INDEX,
    embedding=OpenAIEmbeddings(),
    namespace="memory"
)

# === 4. Charger les √©v√©nements depuis un fichier local ===
def load_evenements_context():
    try:
        with open("evenements_structures.txt", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        return "Aucun √©v√©nement n'a pu √™tre charg√©."

evenements_context = load_evenements_context()

# === 5. Rechercher dans la base de connaissances ===
def get_cci_context(query: str) -> str:
    docs = retriever.invoke(query)
    return "\n\n".join(doc.page_content for doc in docs) if docs else ""

# === 6. Enregistrer un message dans la m√©moire long terme ===
def save_to_long_term_memory(text: str, user_id: str):
    now = datetime.utcnow().isoformat()
    doc = Document(
        page_content=text,
        metadata={
            "user_id": user_id,
            "timestamp": now,
            "type": "user_message"
        }
    )
    long_term_store.add_documents([doc])

# === 7. R√©cup√©rer la m√©moire longue d‚Äôun utilisateur ===
def retrieve_long_term_memory(query: str, user_id: str) -> str:
    docs = long_term_store.similarity_search(query, k=10)
    filtered_docs = [doc for doc in docs if doc.metadata.get("user_id") == user_id]
    return "\n\n".join(doc.page_content for doc in filtered_docs) if filtered_docs else ""

# === 8. M√©moire de conversation courte (session) ===
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory, verbose=False)


# === 6. Fonction principale de l‚Äôagent ===
def agent_response(user_input: str, user_id: str) -> str:
    base_cci_context = get_cci_context(user_input)
    long_term_context = retrieve_long_term_memory(user_input, user_id)

    prompt = f"""
Tu es un assistant intelligent et multilingue de la Chambre de Commerce et d‚ÄôIndustrie Franco-mexicaine. Ton but est d'expliquer et promouvoir les services de la CCI Mexico.
SI un utilisateur te parle en fran√ßais, r√©ponds en fran√ßais. Si c'est en espagnol, r√©ponds en espagnol.


Voici la m√©moire utilisateur long terme :
{long_term_context or '[Aucune m√©moire pertinente pour cet utilisateur.]'}

Voici des informations de la base CCI :
{base_cci_context or '[Pas d‚Äôinformation pertinente dans la base.]'}

Mission
R√©pondre de mani√®re claire, professionnelle et qui incentive √† etre membre en repondant a toutes les questions portant sur :
* les services propos√©s (accompagnement, formations, √©v√©nements, networking, soutien aux entreprises, etc.)
* les conditions et avantages d‚Äôadh√©sion
* les offres r√©serv√©es aux membres
* les partenariats, actualit√©s et √©v√©nements √† venir

Bonnes pratiques

* Quand tu parles d'un service, tu dois indiquer l'URL de la brochure li√©e a ce service, ex : https://drive.google.com/file/d/1sm0IC2Ywfz4WLW2hEbXcGdxY038MXfq8/view?usp=share_link "
* si une information n‚Äôest pas disponible, le pr√©ciser avec courtoisie et orienter l‚Äôutilisateur vers un contact de la CCI

Langue
* d√©tecter automatiquement si la question est pos√©e en fran√ßais ou en espagnol
* r√©pondre int√©gralement dans la langue d√©tect√©e

Style

* ton professionnel, informatif, et legerement promoteur
* ne jamais sortir du p√©rim√®tre de la CCI
  (si la question ne concerne pas la CCI, expliquer poliment que ton r√¥le est uniquement d‚Äôinformer sur la CCI et ses services)

L'utilisateur a dit : "{user_input}"

R√©ponds en fran√ßais, de fa√ßon professionnelle, fluide et utile. Si l'utilisateur pose une question simple (bonjour, merci, etc.), r√©ponds naturellement sans chercher d'information.
"""

    full_prompt = f"{prompt}\nUtilisateur : {user_input}"
    reply = conversation.predict(input=full_prompt)

    # Enregistrement du message utilisateur dans la m√©moire longue
    save_to_long_term_memory(f"Utilisateur : {user_input}\nR√©ponse de l'agent : {reply}", user_id=user_id)

    return reply

# === 10. Boucle de chat ===
if __name__ == "__main__":
    print("ü§ñ Agent CCI (√©v√©nements + base vectorielle + m√©moire longue) ‚Äî pr√™t !\nTape 'exit' pour quitter.\n")
    
    # This need to be changed to the user id based on the web app
    user_id = "lead_001"
    
    while True:
        user_input = input("üí¨ Vous : ")
        if user_input.lower() in ["exit", "quit"]:
            print("üëã √Ä bient√¥t !")
            break
        try:
            reply = agent_response(user_input, user_id=user_id)
            print(f"\nüß† Agent :\n{reply}\n")
        except Exception as e:
            print(f"‚ùå Erreur : {e}")
